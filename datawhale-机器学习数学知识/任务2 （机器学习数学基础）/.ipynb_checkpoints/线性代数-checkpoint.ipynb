{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "drawn-empire",
   "metadata": {},
   "source": [
    "# 2.2 线性代数与python应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-testing",
   "metadata": {},
   "source": [
    "各位同学大家好，本任务的内容是《线性代数》，前一个任务其实我们也是有使用过《线性代数》的部分内容了，只不过我们没有系统学习《线性代数》的内容。那么在本任务中，萌老师将带大家一起走进《线性代数》的世界，探索奇妙而神秘的代数学。本任务的内容包括：\n",
    "   - 线性方程组与向量\n",
    "   - 向量空间、矩阵、行列式以及范数\n",
    "   - 对角化、矩阵的特征值与特征向量、正交化\n",
    "   - 二次型、正定负定的概念与判定\n",
    "   - 线性代数小案例：创建富有浮雕风格的图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-despite",
   "metadata": {},
   "source": [
    "## 2.2.1 线性方程组与向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-gambling",
   "metadata": {},
   "source": [
    "在学习《线性代数》这门课之前，我们先来了解《线性代数》究竟在研究什么？在回答这个问题之前，我们先来回答刚学过的《高等数学》在研究什么这个问题，通俗来说，《高等数学》研究的是函数，而函数研究的是：输入一个数，经过函数运算之后，产出一个数。那回到《线性代数》究竟在研究什么的话题，线性代数研究的是：输入一条直线，经过某些变换加工以后，产出另一条直线。这个话题，大家会随着学习的深入会慢慢深入理解。《线性代数》研究的开始就是为了解线性方程组，因此我们先从线性方程组出发，探索《线性代数》的世界。\n",
    "\n",
    "**线性方程组：**\n",
    "\n",
    "在中学阶段，我们应接触过线性方程组：\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "x+y=4 \\\\\n",
    "x-y=2\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "这个线性方程组在中学阶段叫做二元一次方程组，当然解这个方程组也很简单啊，答案是：$\\left\\{\\begin{array}{l}x=3 \\\\ y=1\\end{array}\\right.$。现在我们忘记中学阶段解二元一次方程组的高斯消元法，把二元一次方程组的本质抽象出来，探讨一种解决二元一次方程组的方法。通过观察，我们发现：二元一次方程组的未知数的阶数都是一次，式子两边都是用等式相连，因此我们给这个方程组起另一个名字：线性方程组。在前面的线性方程组中，我们把同一个未知数的系数放在一起，如把$x$的系数放在一起，就是：1,1。我们回想下中学中，是不是有一个叫向量的东西，可以把两个数放在一起，因此我们把这两个系数放在一起：$[1,1]$。类似的，我们把$y$的系数也放在一起，得到$[1,-1]$。最后，把常数项也放在一起，得到$[4,2]$。总结一下，我们使用熟悉的二维向量的形式展示刚刚的线性方程组：\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "x+y=4 \\\\\n",
    "x-y=2\n",
    "\\end{array}\\right.\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "{1}\\\\\n",
    "{1}\\\\\n",
    "\\end{bmatrix}\n",
    "x+\\begin{bmatrix}\n",
    "{1}\\\\\n",
    "{-1}\\\\\n",
    "\\end{bmatrix}\n",
    "y = \\begin{bmatrix}\n",
    "{4}\\\\\n",
    "{2}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "下面我们尝试使用python解线性方程组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oriental-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性方程组的解为：\n",
      " [3. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "A = np.array([[1.,1],[1,-1]])     # 将系数所有向量拼在一起\n",
    "b = np.array([4,2])  # 常数向量\n",
    "x = np.linalg.solve(A,b)   # 解线性方程组\n",
    "print(\"线性方程组的解为：\\n\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-empty",
   "metadata": {},
   "source": [
    "## 2.2.2 向量空间、矩阵、行列式以及范数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-jenny",
   "metadata": {},
   "source": [
    "现在问题出现了两个：\n",
    "   - (1)能否将二维向量推广至高维向量，因为现实中的影响因素可能成千上万，如果只有几何中的二维向量可能不足以表达实际问题。\n",
    "   - (2)能否找到类似于解决二元一次方程组的高斯消元法一样的，能够解决三元及以上的多元线性方程组的一般解法。\n",
    "\n",
    "### （1）向量空间\n",
    "问题(1)的探讨：能否将二维向量推广至高维向量？\n",
    "\n",
    "先说答案：可以将二维向量推广至高维向量。在初中，我们的向量定义依赖于平面直角坐标系，使用坐标系的话我们没法拓展至高维向量，因为几何空间最多是3维。现在，我们按照二维向量的样子推广至多维向量：\n",
    "$$\n",
    "\\text{二维向量:}\\;[x_1,x_2] \\Rightarrow \\;\\text{n维向量:}\\;[x_1,x_2,...,x_n]\n",
    "$$\n",
    "但是仅仅这样是不够的，因为平面二维向量具有强大作用的原因是拥有自己的运算逻辑：加法，数乘。加法可以让两个向量生成另外一个向量，数乘可以让向量伸缩。\n",
    "   - 加法：$[x_1,x_2] + [y_1,y_2] = [x_1+y_1,x_2+y_2]$\n",
    "   - 数乘：$k[x_1,x_2] = [kx_1,kx_2]$\n",
    "   \n",
    "![jupyter](./picture/5.png)\n",
    "\n",
    "因此，我们也希望定义的高维向量也拥有这个能力：加法与数乘。\n",
    "   - 加法：$[x_1,x_2,...,x_n] + [y_1,y_2,...,y_n] = [x_1+y_1,x_2+y_2,...,x_n+y_n]$\n",
    "   - 数乘：$k[x_1,x_2,...,x_n] = [kx_1,kx_2,...,kx_n]$\n",
    "   \n",
    "做好前面的铺垫后，现在我们可以把空间的概念引进来。在二维空间中，更准确的说在几何空间中，空间存在无数个类似于$[x_1,y_1],[x_2,y_2]$的几何向量，同时空间中还存在控制几何向量之间的相互关系的运算：加法与数乘。举个不太恰当的例子：人类社会中，除了有存在空间中的人以外，还存在着约束每个人的道德与法律。那么n维空间也是一样的，里面存在着无数个n维向量$[x_1,x_2,...,x_n],[y_1,y_2,...,y_n]$，同时存在约束着这些向量运算规则的加法和数乘。\n",
    "\n",
    "### （2）矩阵\n",
    "问题(2)的探讨：能否找到解决任意元线性方程组的一般方法？\n",
    "\n",
    "老规矩，先说答案：线性方程组存在一般解决方法。这个解法需要引入矩阵的概念，为什么需要引入矩阵的概念呢？我们知道，在方程组：\n",
    "$\\left\\{\\begin{array}{l}\n",
    "x+y=4 \\\\\n",
    "x-y=2\n",
    "\\end{array}\\right.$中，每个向量代表了一部分这个方程组的信息，那么我们使用一个表把方程组的系数方面的信息囊括起来，这个表就是矩阵。即：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "{1}\\\\\n",
    "{1}\\\\\n",
    "\\end{bmatrix}\\text{,}\\;\\begin{bmatrix}\n",
    "{1}\\\\\n",
    "{-1}\\\\\n",
    "\\end{bmatrix} \\Rightarrow \\begin{bmatrix}\n",
    "{1}&{1}\\\\\n",
    "{1}&{-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "同时，未知系数本身也可以看作是一个向量:$\\begin{bmatrix}{x}\\\\ {y} \\end{bmatrix}$，因此方程组可以写成如下形式：\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "x+y=4 \\\\\n",
    "x-y=2\n",
    "\\end{array}\\right. \\Rightarrow  \\begin{bmatrix}\n",
    "{1}&{1}\\\\\n",
    "{1}&{-1}\\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}{x}\\\\ \n",
    "{y} \\end{bmatrix} = \\begin{bmatrix}{4}\\\\ {2} \\end{bmatrix}\n",
    "$$\n",
    "虽然，至此我们并没有给出矩阵与向量乘法的定义，但是从方程组的形式可以看出矩阵与向量乘法的定义是：矩阵的每一行乘以向量的列，得到另一个向量。因此，像推广至n维空间一样，我们给出高维矩阵的运算：\n",
    "   - 矩阵的加法：$A=\\left(\\begin{array}{cccc}a_{11} & a_{12} & \\cdots & a_{1 n} \\\\ a_{21} & a_{22} & \\cdots & a_{2 n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{m 1} & a_{m 2} & \\cdots & a_{m n}\\end{array}\\right) \\quad B=\\left(\\begin{array}{cccc}b_{11} & b_{12} & \\cdots & b_{1 n} \\\\ b_{21} & b_{22} & \\cdots & b_{2 n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ b_{m 1} & b_{m 2} & \\cdots & b_{m n}\\end{array}\\right)$，则$A \\pm B=\\left(\\begin{array}{cccc}\n",
    "a_{11} \\pm b_{11} & a_{12} \\pm b_{12} & \\cdots & a_{1 n} \\pm b_{1 n} \\\\\n",
    "a_{21} \\pm b_{21} & a_{22} \\pm b_{22} & \\cdots & a_{2 n} \\pm b_{2 n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "a_{m 1} \\pm b_{m 1} & a_{m 2} \\pm b_{m 2} & \\cdots & a_{m n} \\pm b_{m n}\n",
    "\\end{array}\\right)$\n",
    "   - 矩阵与数的数乘：数 $\\lambda$ 乘矩阵 $A$, 就是将数 $\\lambda$ 乘矩阵 $\\mathrm{A}$ 中的每一个元素, 记为 $\\lambda A$ 或\n",
    "$A \\lambda$\n",
    "   - 结合律: $\\quad(\\boldsymbol{\\lambda} \\boldsymbol{\\mu}) \\mathrm{A}=\\boldsymbol{\\lambda}(\\boldsymbol{\\mu} \\mathrm{A}) ; \\quad(\\boldsymbol{\\lambda}+\\boldsymbol{\\mu}) \\mathrm{A}=\\boldsymbol{\\lambda} \\mathrm{A}+\\boldsymbol{\\mu} \\mathrm{A}$\n",
    "   - 分配律： $\\boldsymbol{\\lambda} (\\mathrm{A}+\\mathrm{B})=\\boldsymbol{\\lambda} \\mathrm{A}+\\boldsymbol{\\lambda} \\mathrm{B}$\n",
    "   \n",
    "讲到这里，已经基本理解了矩阵的部分知识，因此回过头来看线性方程组的一般解法，现在方程组的表达式为：$\\left\\{\\begin{array}{l}\n",
    "x+y=4 \\\\\n",
    "x-y=2\n",
    "\\end{array}\\right. \\Rightarrow  \\begin{bmatrix}\n",
    "{1}&{1}\\\\\n",
    "{1}&{-1}\\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}{x}\\\\ \n",
    "{y} \\end{bmatrix} = \\begin{bmatrix}{4}\\\\ {2} \\end{bmatrix}$。\n",
    "通过观察，我们知道矩阵与向量的乘法最终得到另一个向量，因此我们从这里得到两个重要的信息：\n",
    "   - 矩阵的作用是对一个向量进行变换的，一个矩阵乘一个向量意味着向量经过矩阵的变换变成另一个向量。\n",
    "   - 线性方程组是未知向量经过系数矩阵的变换，变成另一个常数向量。因此，我们只需要反向做一个矩阵变换，常数向量就能变成原来的未知向量，这样未知向量就能求出来了。\n",
    "\n",
    "根据上面得到的信息，这个系数矩阵的反向变换矩阵就是系数矩阵的**逆矩阵**，记作：$\\mathrm{A}^{-1}$，即：$A^{-1}Ax = x = A^{-1}b$。\n",
    "\n",
    "最后，补充说明下矩阵与矩阵的乘法。由于矩阵与向量的乘法的结果是另一个向量，那么矩阵与矩阵的结果就是另外几个向量组成的另外的矩阵。如：\n",
    "$$\n",
    "A=\\left[\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right], B=\\left[\\begin{array}{ccc}\n",
    "1 & 2 & -3 \\\\\n",
    "-1 & 1 & 2\n",
    "\\end{array}\\right]\\\\\n",
    "A B=\\left[\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
    "1 & 2 & -3 \\\\\n",
    "-1 & 1 & 2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
    "-1 & 4 & 1 \\\\\n",
    "2 & 1 & -5\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "因此，矩阵与矩阵的乘法的实际意义就是批量把矩阵中的向量经过另一个矩阵的变换，变成另外几个向量，再组成矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "announced-inflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性方程组的解为：\n",
      " [3. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 使用你矩阵求解线性方程组\n",
    "A = np.array([[1.,1],[1,-1]])     # 将系数所有向量拼在一起\n",
    "b = np.array([4,2])  # 常数向量\n",
    "A_inverse = np.linalg.inv(A)  # 求系数矩阵的逆矩阵\n",
    "x = np.matmul(A_inverse,b)\n",
    "print(\"线性方程组的解为：\\n\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-default",
   "metadata": {},
   "source": [
    "### （3）行列式\n",
    "先看一个式子: $D_{2}=\\left|\\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{array}\\right|$. 我们称其为 2 阶行列式,其中 $a_{i j}$ 的第一个下标 $i$ 表示此元素所在的行数,第二个下标 $j$ 表示此元素所在的列数, $i=1,2, j=1,2$,于是此行列式中有四个元素,并且 $\\left|\\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{array}\\right|=$ $a_{11} a_{22}-a_{12} a_{21} .$ 这是一个什么样的计算规则 $?$ 它背后有什么样的意义?\n",
    "\n",
    "将此行列式的第 1 行的两个元素 $a_{11}, a_{12}$ 看成一个 2 维向量$\\left[a_{11}, a_{12}\\right] \\stackrel{\\text { 记 }}{=} \\boldsymbol{\\alpha}_{1}$，第二行的两个元素 $a_{21}, a_{22}$ 看成另一个 2 维向量 $\\left[a_{21}, a_{22}\\right] \\stackrel{\\text { 记 }}{=} \\boldsymbol{\\alpha}_{2}$。不妨设 $\\boldsymbol{\\alpha}_{1}$ 的长度(模)为 $l, \\boldsymbol{\\alpha}_{2}$ 的长度(模)为 $m, \\boldsymbol{\\alpha}_{1}$ 与 $x$ 轴正向的夹角为 $\\alpha, \\boldsymbol{\\alpha}_{2}$ 与 $x$ 轴正向的夹角为 $\\beta$, 于是,如图所示：\n",
    "\n",
    "![jupyter](./picture/7.png)\n",
    "\n",
    "则：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S_{\\square O A B C} &=l \\cdot m \\cdot \\sin (\\beta-\\alpha) \\\\\n",
    "&=l \\cdot m(\\sin \\beta \\cos \\alpha-\\cos \\beta \\sin \\alpha) \\\\\n",
    "&=l \\cos \\alpha \\cdot m \\sin \\beta-l \\sin \\alpha \\cdot m \\cos \\beta \\\\\n",
    "&=a_{11} a_{22}-a_{12} a_{21}\n",
    "\\end{aligned}\n",
    "$$\n",
    "因此：\n",
    "$$\n",
    "\\left|\\begin{array}{ll}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21}=S_{\\square O A B C}\n",
    "$$\n",
    "我们看到了一个极其直观有趣的结论: 2 阶行列式是由两个 2 维向量组成的,其 $($ 运算规则的)结果为 以这两个向量为邻边的平行四边形的面积。 这不仅得出了 2 阶行列式的计算规则，也能够清楚地看到其几何意义。\n",
    "\n",
    "我们使用python计算行列式的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swedish-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A的行列式的值为： -2.0\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,1,1,1],[1,2,0,0],[1,0,3,0],[1,0,0,4]])\n",
    "A_det = np.linalg.det(A)\n",
    "print(\"A的行列式的值为：\",A_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-spank",
   "metadata": {},
   "source": [
    "行列式的意义不仅仅是衡量面积，还能用来解决线性方程组的求解，这个方法叫做克拉默法则：\n",
    "\n",
    "设线性方程组的表达式为：$\\left\\{\\begin{array}{c}a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n}=b_{1} \\\\ a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n}=b_{2} \\\\ \\cdots \\cdots \\\\ a_{n 1} x_{1}+a_{n 2} x_{2}+\\cdots+a_{n n} x_{n}=b_{n}\\end{array}\\right.$\n",
    "，系数行列式为：$D = \\left|\\begin{array}{cccc}a_{11} & a_{12} & \\cdots & a_{1 n} \\\\ a_{21} & a_{22} & \\cdots & a_{2 n} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ a_{n 1} & a_{n 2} & \\cdots & a_{m n}\\end{array}\\right| \\neq 0$，则该线性方程组有且仅有唯一解:\n",
    "\n",
    "$$\n",
    "x_{1}=\\frac{D_{1}}{D}, x_{2}=\\frac{D_{2}}{D}, \\cdots, x_{n}=\\frac{D_{n}}{D}\n",
    "$$\n",
    "\n",
    "其中，$D_{j}=\\left|\\begin{array}{ccccccc}a_{11} & \\cdots & a_{1, j-1} & b_{1} & a_{1, j+1} & \\cdots & a_{1 n} \\\\ a_{21} & \\cdots & a_{2, j-1} & b_{2} & a_{2, j+1} & \\cdots & a_{2 n} \\\\ \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots \\\\ a_{n 1} & \\cdots & a_{n, j-1} & b_{n} & a_{n, j+1} & \\cdots & a_{n n}\\end{array}\\right|$\n",
    "\n",
    "注: 用克莱姆法则解线性方程组必须有两个前提条件:\n",
    "   - (1)末知数个数等于方程个数\n",
    "   - (2)系数行列式 $D \\neq 0$\n",
    "   \n",
    "例如：解线性方程组 $\\left\\{\\begin{array}{l}2 x_{1}+x_{2}-5 x_{3}+x_{4}=8 \\\\ x_{1}-3 x_{2}-6 x_{4}=9 \\\\ 2 x_{2}-x_{3}+2 x_{4}=-5 \\\\ x_{1}+4 x_{2}-7 x_{3}+6 x_{4}=0\\end{array}\\right.$：\n",
    "\n",
    "解:方程组的系数行列式\n",
    "$$\n",
    "D=\\left|\\begin{array}{cccc}\n",
    "2 & 1 & -5 & 1 \\\\\n",
    "1 & -3 & 0 & -6 \\\\\n",
    "0 & 2 & -1 & 2 \\\\\n",
    "1 & 4 & -7 & 6\n",
    "\\end{array}\\right|=27 \\neq 0\n",
    "$$\n",
    "由克莱姆法则知：方程组有唯一解。\n",
    "\n",
    "$D_{1}=\\left|\\begin{array}{cccc}8 & 1 & -5 & 1 \\\\ 9 & -3 & 0 & -6 \\\\ -5 & 2 & -1 & 2 \\\\ 0 & 4 & -7 & 6\\end{array}\\right|=81 \\Rightarrow x_{1}=\\frac{D_{1}}{D}=\\frac{81}{27} = 3$，$D_{2}=\\left|\\begin{array}{cccc}2 & 8 & -5 & 1 \\\\ 1 & 9 & 0 & -6 \\\\ 0 & -5 & -1 & 2 \\\\ 1 & 0 & -7 & 6\\end{array}\\right|=-108 \\Rightarrow x_{2}=\\frac{D_{2}}{D} =\\frac{-108}{27}= 4$，$D_{3}=\\left|\\begin{array}{cccc}2 & 1 & 8 & 1 \\\\ 1 & -3 & 9 & -6 \\\\ 0 & 2 & -5 & 2 \\\\ 1 & 4 & 0 & 6\\end{array}\\right|=-27 \\Rightarrow x_{3}=\\frac{D_{3}}{D} = =\\frac{-27}{27}=-1$，$D_{4}=\\left|\\begin{array}{cccc}2 & 1 & -5 & 8 \\\\ 1 & -3 & 0 & 9 \\\\ 0 & 2 & -1 & -5 \\\\ 1 & 4 & -7 & 0\\end{array}\\right|=27 \\Rightarrow x_{4}=\\frac{D_{4}}{D} = \\frac{27}{27} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collected-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "克拉默法则解线性方程组的解为：\n",
      " x1=3.00,\n",
      " x2=-4.00,\n",
      " x3=-1.00,\n",
      " x4=1.00\n"
     ]
    }
   ],
   "source": [
    "# 使用python实现克拉默法则：\n",
    "D = np.array([[2.,1,-5,1],[1,-3,0,-6],[0,2,-1,2],[1,4,-7,6]])\n",
    "D_det = np.linalg.det(D)\n",
    "\n",
    "D1 = np.array([[8.,1,-5,1],[9,-3,0,-6],[-5,2,-1,2],[0,4,-7,6]])\n",
    "D1_det = np.linalg.det(D1)\n",
    "\n",
    "D2 = np.array([[2.,8,-5,1],[1,9,0,-6],[0,-5,-1,2],[1,0,-7,6]])\n",
    "D2_det = np.linalg.det(D2)\n",
    "\n",
    "D3 = np.array([[2.,1,8,1],[1,-3,9,-6],[0,2,-5,2],[1,4,0,6]])\n",
    "D3_det = np.linalg.det(D3)\n",
    "\n",
    "D4 = np.array([[2.,1,-5,8],[1,-3,0,9],[0,2,-1,-5],[1,4,-7,0]])\n",
    "D4_det = np.linalg.det(D4)\n",
    "\n",
    "x1 = D1_det / D_det\n",
    "x2 = D2_det / D_det\n",
    "x3 = D3_det / D_det\n",
    "x4 = D4_det / D_det\n",
    "print(\"克拉默法则解线性方程组的解为：\\n x1={:.2f},\\n x2={:.2f},\\n x3={:.2f},\\n x4={:.2f}\".format(x1,x2,x3,x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-partnership",
   "metadata": {},
   "source": [
    "### （4）范数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-blackjack",
   "metadata": {},
   "source": [
    "在向量空间中，为了研究或者度量一个向量近似另一个向量的程度，需要对n维空间中的向量“长度”给出定义，这个定义叫做范数。向量范数是欧几里得空间中的长度概念的一个推广，它对数值分析中的迭代收敛和误差分析等起着重要作用。用最简单的话来说，范数就是衡量向量大小的量，那既然是衡量大小的量，它需要满足以下几个常识性的性质：\n",
    "   - 首先，一个东西的大小应该是非负的，没有负数的大小。\n",
    "   - 其次，一个东西扩大或者缩小k倍后，他的大小也会扩大或者缩小k倍。\n",
    "   - 最后，由于是大小的原因，需要满足三角形两边之和大于第三边。\n",
    "   \n",
    "因此，根据以上的常识性的性质约束，我们给出向量范数的准确定义：\n",
    "\n",
    "设映射 $\\|\\cdot\\|: C^{n} \\rightarrow R$ 满足:\n",
    "   - (1) 正定性 $\\|x\\| \\geq 0$, 当且仅当 $x=0$ 时, $\\|x\\|=0$；\n",
    "   - (2)齐次性 $\\|\\lambda x\\|=|\\lambda|\\|x\\|, \\lambda \\in R, x \\in C^{n}$；\n",
    "   - (3) 三角不等式 $\\|x+y\\| \\leq\\|x\\|+\\|y\\|, \\forall x, y \\in C^{n}$。\n",
    "   \n",
    "则称映射 $\\|\\cdot\\|$ 为 $C^{n}$ 上向量x的范数。\n",
    "\n",
    "观察上面的定义，我们可能会很懵，因为太数学了，但是从数学的定义我们可以知道：首先，范数是一个函数，它可以把一个向量映射到一个值；另外，不是所有的函数都可以叫范数，他必须满足正定性（映射的值代表向量的长度，长度理所当然大于0）、齐次性（将向量放缩$\\lambda$倍，它的长度也应该放缩$\\lambda$倍）和三角不等式性（向量之间是满足两边之和大于第三边的）。最后，萌老师想强调的是：范数不是唯一的函数，有很多种函数都可以拿来充当范数，只要满足上面的定义的函数都可以叫范数，而且不同的范数往往代表的含义不一样。下面，我来介绍几种最常用的范数：\n",
    "\n",
    "（1）$l_0-$范数: \n",
    "$$\n",
    "\\|\\boldsymbol{x}\\|_{0}=\\sqrt[0]{\\sum_{i} x_{i}^{0}}\n",
    "$$\n",
    "$l_0-$范数代表了向量$x$中的非零元素的个数，很多时候在建立数学模型之前的最朴素的模型就是优化$l_0-$范数的模型，这种优化模型在数学上被认为是一个NP-hard问题，即直接求解它很复杂、也不可能找到解。\n",
    "\n",
    "（2）$l_1-$范数：\n",
    "$$\n",
    "\\|\\boldsymbol{x}\\|_{1}=\\sum_{i}\\left|x_{i}\\right|\n",
    "$$\n",
    "$l_1-$范数等于向量中所有元素绝对值之和。优化$l_1-$范数的模型相比优化$l_0-$范数的模型更加简单，借助现有凸优化算法（线性规划或是非线性规划），就能够找到我们想要的可行解。\n",
    "\n",
    "（3）$l_2-$范数：\n",
    "$$\n",
    "\\|\\boldsymbol{x}\\|_{2}=\\sqrt{\\sum_{i} x_{i}^{2}}\n",
    "$$\n",
    "$l_2-$范数表示向量（或矩阵）的元素平方和开根号，是我们常见的计算长度的公式之一，它也叫欧氏距离。优化$l_2-$范数的模型相比前两者是最简单的！\n",
    "\n",
    "（4）$l_p-$范数：\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_{p}=\\sqrt[p]{\\sum_{i=1}^{n} x_{i}^{p}}, \\mathbf{x}=\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)\n",
    "$$\n",
    "$l_p-$范数不是一个范数，而是一组范数,根据P 的变化，范数也有着不同的变化，一个经典的有关P范数的变化图如下： \n",
    "\n",
    "![jupyter](./picture/8.png)\n",
    "\n",
    "(5)$l_{\\infty}$ 范数：\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_{\\infty}=\\max \\left(\\left|x_{i}\\right|\\right)\n",
    "$$\n",
    "$l_{\\infty}$范数，它主要被用来度量向量元素的最大值。\n",
    "\n",
    "下面，我们使用python尝试计算向量的范数吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satellite-pasta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.344080432788601"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 自定义向量范数：\n",
    "import math\n",
    "def Vec_Norm(x,order):\n",
    "    \"\"\"\n",
    "    x:输入的向量，默认是list\n",
    "    order: 0-l0范数； 1-l1范数； 2-l2范数； “inf”-无穷范数\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    if str(order) ==  \"0\":\n",
    "        for i in x:\n",
    "            if i != 0:\n",
    "                value += 1\n",
    "    elif str(order) ==  \"1\":\n",
    "        for i in x:\n",
    "            value += math.fabs(i)\n",
    "    elif str(order) == \"2\":\n",
    "        for i in x:\n",
    "            value += math.pow(i,2)\n",
    "        value = math.sqrt(value)\n",
    "    elif str(order) == \"inf\":\n",
    "        for i in x:\n",
    "            if value <= math.fabs(i):\n",
    "                value = i\n",
    "    else:\n",
    "        print(\"您输入的order参数有误，请重新输入！\")\n",
    "    return value\n",
    "\n",
    "\n",
    "Vec_Norm([1,2,3,8,3,2,4],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recovered-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x的l0范数是： 7.0\n",
      "x的l1范数是： 23.0\n",
      "x的l2范数是： 10.344080432788601\n",
      "x的l-inf范数是： 8.0\n"
     ]
    }
   ],
   "source": [
    "## 使用numpy计算范数\n",
    "x = np.array([1,2,3,8,3,2,4])\n",
    "print(\"x的l0范数是：\",np.linalg.norm(x,0))\n",
    "print(\"x的l1范数是：\",np.linalg.norm(x,1))\n",
    "print(\"x的l2范数是：\",np.linalg.norm(x,2))\n",
    "print(\"x的l-inf范数是：\",np.linalg.norm(x,np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-audit",
   "metadata": {},
   "source": [
    "## 2.2.3对角化、矩阵的特征值与特征向量、正交化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-advancement",
   "metadata": {},
   "source": [
    "在我们刚刚对矩阵的讨论中，我们知道：一个矩阵对应着一个变换，也就是说一个矩阵能把一个向量变成另一个向量，即：$Ax=b$。我们提及一个重要的核心概念：向量在空间中的位置是绝对的，而其坐标值却是相对的，坐标的取值依托于其所选取的坐标向量（基底）。更直白的说就是，对于同一个向量，选取的坐标向量（基底）不同，其所对应的坐标值就不同。(下图来源于石溪)\n",
    "\n",
    "![jupyter](./picture/9.jpeg)\n",
    "\n",
    "从途中我们可以看到：向量$a$在直角坐标系下与在基底$e_1^{'},e_2^{'}$下的坐标显然是不同。下面，我们来正式介绍一个概念：相似矩阵。\n",
    "\n",
    "假设一个向量在坐标系1下表示的坐标为$x$，当这个向量$x$经过一个线性变换形成一个新的向量$y$，用矩阵表示这个变换就是：$y = Ax$，矩阵$A$对应着$x \\rightarrow y$的线性变换。同时，向量也可以在坐标系2下表示，其坐标为$x^{'}$，那么$x^{'} = Px$。同理，$x^{'}$也可以经过同一个线性变换变成$y^{'}$，即：$y^{'} = Bx^{'}=BPx$。最后我们把$y^{'}$转化为同一个坐标系下表达，即$y=P^{-1}y^{'}=P^{-1}BPx$。因此，我们可以得到：$Ax = P^{-1}BPx$，即：\n",
    "$$\n",
    "A = P^{-1}BP\n",
    "$$\n",
    "我们称满足上式的矩阵A、B称为相似矩阵。总结一下：一个向量在空间位置里，选取不同的坐标系，其坐标值是不同的。对于空间中同一个线性变换，在不同的坐标系下，用于描述这个变换的矩阵也是不同的, 而这些不同矩阵所描述的线性变换是相似的，因此我们称他们为相似矩阵。\n",
    "\n",
    "那知道相似矩阵的概念有什么用呢？一个矩阵代表着一个线性变换，而不同的坐标系又会得到不同的相似矩阵，那我们能不能选用一个最佳的坐标系，使得我们描述的这个线性变换的矩阵是最佳的呢？什么矩阵才能称得上是最佳矩阵呢？答案就是对角矩阵！因为当我们同时需要经历很多次线性变换的时候，对角矩阵能极大的减少我们的计算量，即：\n",
    "$$\n",
    "A^{n}=\\left[\\begin{array}{lll}\n",
    "a_{1} & & \\\\\n",
    "& a_{2} & \\\\\n",
    "& & a_{3}\n",
    "\\end{array}\\right]^{n}=\\left[\\begin{array}{lll}\n",
    "a_{1}^{n} & & \\\\\n",
    "& a_{2}^{n} & \\\\\n",
    "& & a_{3}^{n}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "那我们如何通过相似矩阵构造对角矩阵呢？我们希望通过相似矩阵的概念找到一个对角矩阵$\\Lambda=\\operatorname{diag}\\left(\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{n}\\right)$，即：$P^{-1}AP = \\Lambda$。首先，矩阵 $P$ 和 $A$ 一样，均为 $n$ 阶方阵。为了方便分析和描述，我们把矩阵$P$写成一组列向量并排 排列的形式： $P=\\left[\\begin{array}{llll}p_{1} & p_{2} & \\ldots & p_{n}\\end{array}\\right]$, 即 $n$ 个 $n$ 维列向量的横向排列。根据$P^{-1}AP = \\Lambda$，我们左乘一个矩阵$P$，得到：$A P=P \\Lambda$，具体展开：\n",
    "$$\n",
    "A\\left[p_{1}, p_{2}, \\ldots, p_{n}\\right]=\\left[p_{1}, p_{2}, \\ldots, p_{n}\\right]\\left[\\begin{array}{llll}\n",
    "\\lambda_{1} & & & \\\\\n",
    "& \\lambda_{2} & & \\\\\n",
    "& & \\cdots & \\\\\n",
    "& & & \\lambda_{n}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "进而可以得到：$\\left[A p_{1}, A p_{2}, \\ldots, A p_{n}\\right]=\\left[\\lambda_{1} p_{1}, \\lambda_{2} p_{2}, \\ldots, \\lambda_{n} p_{n}\\right]$。那么问题的答案就出来了：为了上面这个等式能成立, 就必须让左右两边的向量在每个维度上分别相等。即, $A p_{1}=\\lambda_{1} p_{1}, \\quad A p_{2}=\\lambda_{2} p_{2}, \\ldots, \\quad A p_{n}=\\lambda_{n} p_{n}$ 。\n",
    "\n",
    "总结一下：\n",
    "\n",
    "第一步是：我们要找到满足上述等式$A p_{1}=\\lambda_{1} p_{1}, \\quad A p_{2}=\\lambda_{2} p_{2}, \\ldots, \\quad A p_{n}=\\lambda_{n} p_{n}$的这一组向量 $p_{1}, p_{2}, \\ldots, p_{n}$ 。找到他们之后，我们将其横向排列，就构成了我们苦心寻找的转换矩阵 $P=\\left[\\begin{array}{llll}p_{1} & p_{2} & \\ldots & p_{n}\\end{array}\\right]$;\n",
    "\n",
    "第二步是：将分别与向量 $p_{1}, p_{2}, \\ldots, p_{n}$ 对应的值 $\\lambda_{1}, \\lambda_{2}, \\ldots \\lambda_{n}$ 依序沿着对角线排列，就构成 了与矩阵 $A$ 相似的对角矩阵 $\\Lambda=\\left[\\begin{array}{cccc}\\lambda_{1} & & & \\\\ & \\lambda_{2} & & \\\\ & & . & \\\\ & & & \\lambda_{n}\\end{array}\\right]$ 。\n",
    "\n",
    "那么对角化的问题就直接转化为了：如何找到满足等式$A p_{1}=\\lambda_{1} p_{1}, \\quad A p_{2}=\\lambda_{2} p_{2}, \\ldots, \\quad A p_{n}=\\lambda_{n} p_{n}$的一组向量$p_{1}, p_{2}, \\ldots, p_{n}$和对应的值$\\lambda_1,\\lambda_2,...,\\lambda_n$。首先，我们的等式为：$Ap = \\lambda p$，那么$Ap = \\lambda Ip$，$I$为单位矩阵。我们稍作变形：$(A-\\lambda I)p = 0$，那么如果这个$p$是有解的话，那么$A-\\lambda I$的行列式$det(A-\\lambda I)=0$。因此我们只需要解这个方程$det(A-\\lambda I)=0$就可以求出$\\lambda$和向量$p$了。\n",
    "\n",
    "重点来了：我们把满足$Ap = \\lambda p$的数值$\\lambda$为矩阵$A$的特征值，称$p$为矩阵$A$关于特征值$\\lambda$的特征向量。那特征值和特征向量有什么意义呢？不难看出，由于$Ap = \\lambda p$，而一个矩阵对应一个线性变换，因此经过矩阵A变换后的向量竟然是原向量的伸缩，因此特征向量就是那些经过矩阵A变换后的向量方向与变换前的方向相同或者相反的向量。\n",
    "\n",
    "最后给一个例子给大家演示下怎么求特征值和特征向量吧！\n",
    "\n",
    "求矩阵 $\\boldsymbol{A}=\\left(\\begin{array}{ccc}-\\mathbf{1} & \\mathbf{1} & \\mathbf{0} \\\\ -4 & 3 & 0 \\\\ 1 & 0 & 2\\end{array}\\right)$ 的特征值和特征向量。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "|A-\\lambda E| &=\\left|\\begin{array}{ccc}\n",
    "-1-\\lambda & 1 & 0 \\\\\n",
    "-4 & 3-\\lambda & 0 \\\\\n",
    "1 & 0 & 2-\\lambda\n",
    "\\end{array}\\right|=(2-\\lambda)\\left|\\begin{array}{cc}\n",
    "-1-\\lambda & 1 \\\\\n",
    "-4 & 3-\\lambda\n",
    "\\end{array}\\right| \\\\\n",
    "&=(2-\\lambda)(\\lambda-1)^{2}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "特征值为 $\\lambda=\\mathbf{2}, \\mathbf{1}$。\n",
    "\n",
    "把每个特征值 $\\boldsymbol{\\lambda}$ 代入线性方程组 $(A-\\lambda E) x=0$， 求出基础解系。当 $\\lambda=2$ 时, 解线性方程组 $(A-2 E) x=0$。\n",
    "$$\n",
    "(A-2 E)=\\left(\\begin{array}{lll}\n",
    "-3 & 1 & 0 \\\\\n",
    "-4 & 1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right) \\rightarrow\\left(\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "$\\left\\{\\begin{array}{l}x_{1}=0 \\\\ x_{2}=0\\end{array} \\quad\\right.$ 得基础解系: $p_{1}=\\left(\\begin{array}{l}0 \\\\ 0 \\\\ 1\\end{array}\\right)$\n",
    "\n",
    "当 $\\lambda=1$ 时, 解线性方程组 $(A-E) x=0$：\n",
    "$$\n",
    "(A-E)=\\left(\\begin{array}{ccc}\n",
    "-2 & 1 & 0 \\\\\n",
    "-4 & 2 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{array}\\right) \\rightarrow\\left(\\begin{array}{lll}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "则：\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "x_{1}+x_{3}=0 \\\\\n",
    "x_{2}+2 x_{3}=0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "得基础解系\n",
    "$$\n",
    "p_{2}=\\left(\\begin{array}{c}-1 \\\\ -2 \\\\ 1\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "refined-madonna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵A的特征值为： [-1.  2.  2.]\n",
      "矩阵A的特征向量为：\n",
      " [[-0.70710678 -0.24253563  0.30151134]\n",
      " [ 0.          0.          0.90453403]\n",
      " [-0.70710678 -0.9701425   0.30151134]]\n",
      "矩阵A对角化为：\n",
      " [[-1.00000000e+00 -4.64569451e-16 -3.91625251e-16]\n",
      " [ 4.89706803e-17  2.00000000e+00 -3.82231840e-17]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 使用python求解矩阵的特征值和特征向量\n",
    "A = np.array([[-2,1,1],\n",
    "             [0,2,0],\n",
    "             [-4,1,3]])\n",
    "lamb,p = np.linalg.eig(A)\n",
    "print(\"矩阵A的特征值为：\",lamb)\n",
    "print(\"矩阵A的特征向量为：\\n\",p)\n",
    "print(\"矩阵A对角化为：\\n\",np.matmul(np.linalg.inv(p),np.matmul(A,p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-publisher",
   "metadata": {},
   "source": [
    "在刚刚的讨论中，我们了解了一种十分神奇的向量叫特征向量，这个向量可以在某个矩阵的变换下保持在同一直线上，也就是没有发生角度的偏转。那好奇的我们又开始想问题了，有没有一个矩阵是可以做到令一个向量进行旋转变换或者镜像变换呢？仔细思考下可以发现，这两种变换并没有改变向量的长度，而刚刚的特征向量反而与原向量的关系是拉伸（缩短）的关系。那令一个向量进行旋转变换或者镜像变换的矩阵是什么呢？答案就是：正交矩阵。\n",
    "\n",
    "如果满足以下等式的矩阵$A$称为正交矩阵：\n",
    "$$\n",
    "A^TA = AA^T = I\n",
    "$$\n",
    "通过上式，我们很快能知道：正交矩阵的逆矩阵与转置矩阵相同，即：$A^{-1} = A^T$，因为：$A^{-1}A=I$。同时，正交矩阵有一个非常好的性质：A的各行是单位向量且两两正交（垂直），又或者说A的各列是单位向量且两两正交（垂直），为什么呢？为了说明这个性质，我们将矩阵$A$分成n个行向量或者n个列向量，即：\n",
    "$$\n",
    "A=\\left(\\begin{array}{c}\n",
    "\\alpha_{1} \\\\\n",
    "\\alpha_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_{n}\n",
    "\\end{array}\\right)=\\left(\\beta_{1}, \\beta_{2}, \\cdots, \\beta_{n},\\right)\n",
    "$$\n",
    "如果你实在是想不出来，可以看下面的解释：\n",
    "\n",
    "![jupyter](./picture/10.png)\n",
    "\n",
    "按照矩阵乘法的定义，可以得到行向量跟列向量（行向量的转置）是互相正交（垂直）的。即：\n",
    "\n",
    "![jupyter](./picture/11.png)\n",
    "\n",
    "因此，我们可以得到以下规律：\n",
    "   - $\\alpha_{i} \\cdot \\alpha_{i}^{T}=1$，也就是说$\\alpha_{i} $是单位向量。\n",
    "   - $\\alpha_{i} \\cdot \\alpha_{j}^{T}=0 \\quad(i \\neq j)$，向量之间两两正交。\n",
    "\n",
    "说到这里，已经有人能看出来，最简单的一个正交矩阵是单位矩阵，我们可以将它想象成一个坐标轴，而别的正交矩阵可以由这个坐标轴绕着原点进行旋转或者镜像得到，如图：\n",
    "\n",
    "![jupyter](./picture/12.png)\n",
    "\n",
    "到现在为止，我们已经很显然发现了一个结论：$det(A) = 1 \\;or\\; -1$，因为正交矩阵可以由坐标轴组成的正方体旋转或者镜像变换而来，行列式的绝对值描述了向量围成的面积/体积，因此正交矩阵的行列式要么是1，要么是-1。如果是数学推导的话，即：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&1=\\operatorname{det}(I)=\\operatorname{det}\\left(A^{T} A\\right)=\\operatorname{det}\\left(A^{T}\\right) \\operatorname{det}(A)=\\operatorname{det}(A)^{2} \\\\\n",
    "&\\Rightarrow \\operatorname{det}(A)=\\pm 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "正交矩阵我们介绍完了，那么我们为什么需要进行正交化呢？因为正交化后能更容易进行运算，如求逆矩阵。如果正常矩阵求逆，则计算量很大，而正交矩阵的逆矩阵就是转置。我们如何从一个矩阵得到一个完美的正交矩阵呢？方法叫施密特正交化，由于原理不难理解，这里我给出参考文案供大家使用：\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/136627868\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guilty-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -1.78570734e-16 -3.75993777e-16]\n",
      " [-1.78570734e-16  1.00000000e+00 -3.95178450e-16]\n",
      " [-3.75993777e-16 -3.95178450e-16  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 将矩阵正交化\n",
    "from scipy.linalg import *\n",
    "A = np.array([[1,2,3],[2,1,3],[3,2,1]])\n",
    "B = orth(A)  # 正交化，奇异值分解不是施密特正交化\n",
    "print(np.matmul(B,np.transpose(B)))   # 输出单位矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-airfare",
   "metadata": {},
   "source": [
    "## 2.2.4 二次型、正定负定的概念与判定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-updating",
   "metadata": {},
   "source": [
    "在我们刚刚的讨论中，所有的讨论都是线性的讨论，那么至此我们想将目光投放到更加高阶上，最简单的高阶就是二次，因此我们来研究一个新的概念：二次型。先用一个例子讲明白什么是二次型：\n",
    "\n",
    "考虑方程 $\\frac{13}{72} x^{2}+\\frac{10}{72} x y+\\frac{13}{72} y^{2}=1$ 在平面上代表什么曲线?先把图画出来：\n",
    "\n",
    "![jupyter](./picture/13.png)\n",
    "\n",
    "从x-y坐标系来看，我们能看出方程所表示的图形的确是椭圆，但是这个椭圆的实轴与虚轴并不在我们常规意义上的x和y轴，如果我们能把x-y坐标系旋转$45^{。}$，即：\n",
    "$$\n",
    "x=\\frac{\\sqrt{2}}{2} u+\\frac{\\sqrt{2}}{2} v \\quad y=-\\frac{\\sqrt{2}}{2} u+\\frac{\\sqrt{2}}{2} v\n",
    "$$\n",
    "就是我们常规意义上的椭圆，即$\\mu-v$坐标轴：$\\frac{u^{2}}{9}+\\frac{v^{2}}{4}=1$。\n",
    "\n",
    "现在，我们来看看二次型的数学定义：\n",
    "\n",
    "含有n个变量 $x_{1}, x_{2}, \\ldots, x_{n}$ 的二次齐次函数\n",
    "$f\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)=a_{11} x_{1}^{2}+a_{22} x_{2}^{2}+\\ldots+a_{n n} x_{n}^{2}+2 a_{12} x_{1} x_{2}+2 a_{13} x_{1} x_{3}+\\ldots+2 a_{n-1, n} x_{n-1} x_{n}$ 称为二次型。\n",
    "只含有平方项的二次型 $f=k_{1} y_{1}^{2}+k_{2} y_{2}^{2}+\\ldots+k_{n} y_{n}^{2}$ 称为二次型的标准形。\n",
    "\n",
    "我们使用矩阵的形式来表示二次型：\n",
    "\n",
    "$f\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)=a_{11} x_{1}^{2}+a_{22} x_{2}^{2}+\\ldots+a_{n n} x_{n}^{2}+2 a_{12} x_{1} x_{2}+2 a_{13} x_{1} x_{3}+\\ldots+2 a_{n-1, n} x_{n-1} x_{n}=$\n",
    "$\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\\left(\\begin{array}{cccc}a_{11} & a_{12} & \\ldots & a_{1 n} \\\\ a_{21} & a_{22} & \\ldots & a_{2 n} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n 1}\\end{array}\\right)\\left(\\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\ldots \\\\ x_{n}\\end{array}\\right)$\n",
    "\n",
    "\n",
    "记 $A=\\left(\\begin{array}{cccc}a_{11} & a_{12} & \\ldots & a_{1 n} \\\\ a_{21} & a_{22} & \\ldots & a_{2 n} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n 1}\\end{array}\\right), x=\\left(\\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\cdots \\\\ x_{n}\\end{array}\\right)$\n",
    "\n",
    "则二次型可记作 $f=x^{T} A x$, 其中 $A$ 为对称矩阵。\n",
    "\n",
    "例如：写出二次型\n",
    "$$\n",
    "f=x_{1}^{2}+2 x_{2}^{2}-3 x_{3}^{2}+4 x_{1} x_{2}-6 x_{2} x_{3}\n",
    "$$\n",
    "的矩阵。\n",
    "\n",
    "$\\begin{aligned}\n",
    "&a_{11}=1, a_{22}=2, a_{33}=-3 \\\\\n",
    "&a_{12}=a_{21}=2, \\quad a_{13}=a_{31}=0, \\\\\n",
    "&a_{23}=a_{32}=-3 . \\\\\n",
    "&\\quad A=\\left(\\begin{array}{ccc}\n",
    "1 & 2 & 0 \\\\\n",
    "2 & 2 & -3 \\\\\n",
    "0 & -3 & -3\n",
    "\\end{array}\\right)\n",
    "\\end{aligned}$\n",
    "\n",
    "二次型的标准形对应的矩阵是对角矩阵。\n",
    "$$\n",
    "f=d_{1} y_{1}^{2}+d_{2} y_{2}^{2}+\\ldots+d_{n} y_{n}^{2}=\\left(\\begin{array}{llll}\n",
    "y_{1} & y_{2} & \\ldots & y_{n}\n",
    "\\end{array}\\right)\\left(\\begin{array}{cccc}\n",
    "d_{1} & 0 & 0 & 0 \\\\\n",
    "0 & d_{2} & 0 & 0 \\\\\n",
    "0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & 0 & d_{n}\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\ldots \\\\\n",
    "y_{n}\n",
    "\\end{array}\\right)=y^{T} D y\n",
    "$$\n",
    "\n",
    "在二次型中，也是分为很多种不同类型的二次型的，如：正定二次型、负定二次型，不定二次型等等，例如：\n",
    "\n",
    "正定二次型：$f\\left(x_{1}, x_{2}, x_{3}\\right)=x_{1}^{2}+2 x_{2}^{2}+5 x_{3}^{2}$             \n",
    "半正定二次型：$f\\left(x_{1}, x_{2}, x_{3}\\right)=x_{1}^{2}+2 x_{2}^{2}$            \n",
    "负定二次型：$f\\left(x_{1}, x_{2}, x_{3}\\right)=-x_{1}^{2}-2 x_{2}^{2}-5 x$            \n",
    "半负定二次型：$f\\left(x_{1}, x_{2}, x_{3}\\right)=-x_{2}^{2}-5 x_{3}^{2}$              \n",
    "不定二次型：$f\\left(x_{1}, x_{2}, x_{3}\\right)=x_{1}^{2}-x_{2}^{2}-5 x_{3}^{2}$\n",
    "\n",
    "为什么要对二次型进行区分呢？因为在很多应用上，不同类型的二次型发挥着不同的作用。例如：在判断函数中某个点是否是极小值点的时候，如果该点的二阶导数矩阵（海森矩阵）是正定的话，那么该点事极小值点（可以想象成开口向上的二次函数的最小值点）。那如何判断一个二次型的类型呢？\n",
    "\n",
    "下面，萌老师以正定二次型为例子判断二次型是否是正定的。\n",
    "   - （1）写出二次型的矩阵，这个矩阵是**对称矩阵**。\n",
    "   - （2）求出这个矩阵的**所有**特征值，注意事所有特征值。\n",
    "   - （3）如果该矩阵所有的特征值为正数，那么该矩阵事正定矩阵，该二次型为正定二次型。\n",
    "   \n",
    "下面，我们使用python判断$f\\left(x_{1}, x_{2}, x_{3}\\right)=x_{1}^{2}+6 x_{2}^{2}+5 x_{3}^{2}+4 x_{1} x_{2}+2 x_{1} x_{3}+2 x_{2} x_{3}$是否是正定二次型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecological-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A的所有特征值是否大雨0: True\n"
     ]
    }
   ],
   "source": [
    "# 写出二次型的矩阵：对称矩阵\n",
    "A = np.array([[1,2,1],\n",
    "             [2,6,1],\n",
    "             [1,1,5]])\n",
    "\n",
    "# 求出矩阵A的所有特征值\n",
    "fea = np.linalg.eigvals(A)\n",
    "\n",
    "# 判断所有特征值是否全部大于0\n",
    "print(\"A的所有特征值是否大雨0:\",np.all(fea>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-arrangement",
   "metadata": {},
   "source": [
    "## 2.2.5 线性代数小案例：创建富有浮雕风格的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "首先先把图片读入内存，\n",
    "转化为灰度图像，\n",
    "使用当前像素值 = 相邻像素值之差来得到图像的边缘特征，在加上固定数值150就可以得到浮雕效果了。\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img = cv2.imread('./picture/14.jpeg',1)\n",
    "#获取图像的高度和宽度\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "#图像灰度处理\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#创建目标图像\n",
    "dstImg = np.zeros((height,width,1),np.uint8)\n",
    "\n",
    "#浮雕特效算法：newPixel = grayCurrentPixel - grayNextPixel + 150\n",
    "for i in range(0,height):\n",
    "    for j in range(0,width-1):\n",
    "        grayCurrentPixel = int(gray[i,j])\n",
    "        grayNextPixel = int(gray[i,j+1])\n",
    "        newPixel = grayCurrentPixel - grayNextPixel + 150\n",
    "        if newPixel > 255:\n",
    "            newPixel = 255\n",
    "        if newPixel < 0:\n",
    "            newPixel = 0\n",
    "        dstImg[i,j] = newPixel\n",
    "        \n",
    "#显示图像\n",
    "cv2.imshow('src', img )\n",
    "cv2.imshow('dst',dstImg )\n",
    "\n",
    "#等待显示\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-sister",
   "metadata": {},
   "source": [
    "恭喜大家，完成了所有关于线性代数的内容，下一个章节，我们将进入概率论与数理统计的世界，大家加把劲，我们下一个任务见！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('pytorch_env': conda)",
   "language": "python",
   "name": "python39264bitpytorchenvconda57c06677e0664d478b4a2aabec27deb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
