# 动手学深度学习

> 之前学习深度学习和机器学习看的很杂，很多概念都没有好好理解到位。计算机领域更加强调知行合一。这里记录一下学习李沐的学习过程，希望可以帮助自己提升个人的学业水平、整理总结能力、coding能力。

### 深度学习的介绍

![image-20211019111841822](动手学深度学习.assets/image-20211019111841822.png)

过去十年最热的方向，也是本次学习的重点。利用机器学习处理NLP、CV领域。$x$轴代表随着时间的推移，处理方法的变化。$y$轴代表 其处理范围的不同，可以看出不断沿着AI的方向发展。

应用领域及其广泛，比如图片分类、风格迁移、广告推荐等

当前深度学习处理领域职业主要可分为：数据科学家、AI专家、领域专家

##### Q & A

1. 在机器学习的图片分割领域，为什么有效，模型的可解释性怎么样？

   - 对于DL，这一方面做的不太好，相当于"黑盒"
   - 对于ML，有些模型有一定的解释性。
   - 有效和可解释性不一样。针对特定的模型，在一个特定领域运用会有特定的解释，比如考虑空间信息，时间信息。可解释性目前比较欠缺，有待学术上的进一步进展。

2.  深度学习不能用数学表述，只能使用直觉理解是吗

   这个是不一定的，有些部分开始数学进行表示。但是目前来说使用数学进行表示，是做的不太好的地方。

3.  符号学会应用在深度学习中吗？

   会的，在比较复杂的深度学习模型，现在会用到部分符号学的推理。

4. mac是否可以使用pytorch

   mac可以使用，但是目前不可以使用gpu来操作pytorch

5. 如何寻找自己领域paper？

   后面会专门介绍

6. 无人驾驶在深度学习的应用

   无人驾驶属于错误了一次，造成后果很严重的领域。目前使用深度学习是多模型的融合。

### 动手安装

学习最高境界——实践教学

李沐老师这里是使用Amazon云安装

[视频安装教程](https://zh-v2.d2l.ai/chapter_installation/index.html)



### 数据操作

N维数组样列

- 1维：标量
- 2维：向量
- 3维：RGB图片(宽x高x通道)
- 4维：一个RGB图片批量(批量大小x宽x高x通道)
- 5维：一个视频的批量（批量大小x时间x宽x高x通道)



创建数组：

![image-20211019151845093](动手学深度学习.assets/image-20211019151845093.png)



最后一个代表：行是三行一跳，列是2列一跳

> Q & A

- 是否要先学习线性代数
  - 不用，如果可以，可以学习python的Numpy
- Tensor 和 列表一样吗
  - 不一样，Tensor是指的是数学意义上的“张量”
- view() reshape() 是否一样
  - 没有本质区别





#### 线性代数

- 详情见 [线性代数基本知识](https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_5.pdf)

- 线性代数的常规操作

  - torch定义矩阵与向量、矩阵与矩阵的乘法

    ```
    import torch
    B = torch.arange(20).reshape(5,4)
    a = torch.arange(4)
    A = torch.arange(12).reshape(4, 3)
    print(a, B)
    print(torch.mv(B,a))
    print(torch.mm(B, A))
    ```

  - 范数

    ```
    # L1范数
    u = torch.tensor([-3, -4], dtype=float)
    print(torch.abs(u).sum())
    # L2范数
    print(torch.norm(u))
    # 矩阵的Frobenius norm范数
    print(torch.norm(torch.ones(4, 9)))
    ```

  - axis = 0 求sum() 代表对列求和， axis = 1 求sum() 代表对行求和

    当shape为三维时，shape [2, 5, 4]，如果对axis=1求 sum()，则得到为shape[2, 1, 4],也可以对多个维度求sum()

    ```
    A = torch.arange(20).reshape(2, 2, 5)
    print(A.sum())
    print('-----------')
    sum_A_axis0 = A.sum(axis=0)
    print(sum_A_axis0.shape)
    print('------------------------------')
    sum_A_axis1 = A.sum(axis=2)
    print(sum_A_axis1.shape)
    print('------------------------------')
    sum_A_axis1_keep = A.sum(axis=2, keepdim=True)
    print(sum_A_axis1_keep.shape)
    print('------------------------------')
    
    #  output:
    '''
    torch.Size([])
    -----------
    torch.Size([2, 5])
    ------------------------------
    torch.Size([2, 2])
    ------------------------------
    torch.Size([2, 2, 1])
    ------------------------------
    '''
    ```

- Q & A

  - 对数据进行张量的转换有什么负面的影响？比如数据变得稀疏
    - 比如字符串，转换的话，会生成很多0，这样的话会加大存储容器。变成稀疏矩阵，对深度学习来说没有什么影响
  - 为什么 深度学习要用张量表示
    - 深度学习由机器学习发展过来，机器学习偏向于统计学和计算机的融合，最后深度学习也借助了这一概念
  - copy 与 clone
    - clone是一定复制
  - 对那维度求和就是消除那维度吗
    - 可以这样理解
  - torch 不区分行向量和列向量
    - 可以使用维度来区分
  - sum(axis=[0, 1]) 怎么样求
    - 不知道，哈哈哈
  - 稀疏的时候可以它当成单词做词向量解决吗
    - 可以的，在NLP领域中会重要运用

  

  #### 矩阵计算

  > 标量导数

  ![image-20211111211758611](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111211758611.png)

  > 梯度

  图中代表对x,y求导情况下，所得的导数的性质
  
  ![image-20211111212001543](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111212001543.png)
  
  梯度指向值变化值最大的方向
  
  ![image-20211111212442860](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111212442860.png)
  
- 分子布局符号（分母布局符号）：这里要注意的是，黄色矩形的形状

  - ![image-20211111212844307](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111212844307.png)

- ![image-20211111213139598](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111213139598.png)

- ![image-20211111213535513](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111213535513.png)

- 导数作用主要是进行梯度下降，很容易陷入局部最优解？请问是否可以通过方法使得下降达到全局最优解

  - 一般来说不可以，除非是凸函数。但是深度学习、机器学习不会处理凸函数问题。不关心P问题，只关心NP问题

- Pytorch MXNet 采用的是自动微分和计算图吧，不会手算吧

  - 不会，但是我们要了解基本的深度学习求导过程



> 向量链式法则

![image-20211111214135648](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111214135648.png)

![image-20211111214448128](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111214448128.png)



![image-20211111214613485](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111214613485.png)

> 自动求导

神经网络的话，动不动就是几百层，所以手动求导很复杂。有别于 `符号求导` `数值求导`

- 计算图——等价于使用链式求导的过程

  - ![image-20211111214902425](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111214902425.png)
  - 自动求导的两种方式
    - ![image-20211111215144856](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111215144856.png)
  - ![image-20211111215321256](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111215321256.png)
  - ![image-20211111215507686](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111215507686.png)
  - ![image-20211111215804787](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211111215804787.png)

  ```
  import torch
  x = torch.arange(4.0)
  
  # y 关于 x的导数放在 x.grad
  x.requires_grad_(True)  # 等价于 x = torch.arange(4.0, requires_grad = True)
  x.grad
  y = 2 * torch.dot(x, x)
  print(y)
  #tensor(28., grad_fn=<MulBackward0>)
  y.backward()
  x.grad
  x.grad == 4 * x
  #tensor([True, True, True, True])
  ```

  x.grad.zero_()的作用

  ```
  #x.grad.zero_()  #_下划线pytorch代表 重写我的内容。这里进行zero()是为了防止梯度叠加
  x.grad.zero_()
  y = x.sum()
  y.backward()
  x.grad
  ```

- 固定参数，移除某些计算到计算图外部

  ```
  # 在深度学习中，目的不是为了计算微分矩阵。而是对标量求导，而是批量中每个样本单独计算的偏导数之和
  # 把某些计算移动到记录的计算图之外
  x.grad.zero_()
  y = x * x
  # 把u == y作为一个常数，和x无关. 后面会用到把参数固定
  u = y.detach()
  z = u * x
  print(z,type(z))
  # 这里如果使用 z.backward() 是不行的
  #  grad can be implicitly created only for scalar outputs
  z.sum().backward()
  x.grad == u
  #tensor([True, True, True, True])
  ```

构建函数的计算图需要通过Python控制流，我们仍然可以计算得到的梯度

```
def f(a):
  b = a * 2
  while b.norm() < 1000:
    b = b * 2
  if b.sum() > 0:
    c = b
  else:
    c = 100 * b
  return c

# size=() 代表产生一个标量
a = torch.randn(size=(), requires_grad=True)
print('a:       ', a)
d = f(a)
print(d)
d.backward()

a.grad == d/a
```

#### 自动求导Q & A

- 需要正向和反向都算一遍吗？
  - 神经网络中正向计算 $y$ ，反向得到梯度
- 为什么pytorch会默认累积梯度？
  - 通常需要累积梯度，pytorch对于内存消耗较大，pytorch影响对内存管理不是很好。一般是使用批量切割。batchsize
- 为什么深度学习一般是对标量进行求导，而不是对矩阵或者向量
  - 深度学习中，loss一般是一个标量，极少情况下是一个向量，所以一般对于标量求导
- 多个loss分别反向的时候是不是要累加梯度
  - 是的
- 为什么获取.grad要使用backward()
  - 如果你不使用backward()，它不会计算梯度。因为计算梯度是一个很贵的事情
- 求导过程是不是都是有向图，也就是可以用树状表示，有没有其他环状的图结构
  - 有的，比如RNN中就是使用循环图的结构，但是我们还是会把环状图展开形成树状计算



### 线性回归

> 线性模型

![image-20211113105437885](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211113105437885.png)

以房价预测为例子：

- ![image-20211113105700932](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211113105700932.png)
- ![image-20211113105714024](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211113105714024.png)
- ![image-20211113110016094](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211113110016094.png)
- 总结
  - 线性回归是对$n$维输入的加权，外加偏差
  - 使用平方损失来权衡预测值和真实值的差异
  - 线性回归有显示解——==一般深度学习没有显示解==
  - 线性回归可以看作一个单层的神经网络

#### 优化方法

- 梯度下降：

  - ![image-20211113110358718](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211113110358718.png)
  - 学习率的选择
    - 如果太小、太大：
      - ![image-20211113110616735](E:\研究生\工作\Datawhale\study-in-graduate\动手学深度学习\动手学深度学习.assets\image-20211113110616735.png)

- 小批量随机梯度下降——深度学习基本不使用==梯度下降==

  - 在整个训练集计算梯度"太贵"

    - 一个深度模型可能需要数分钟到数小时

  - 我们可以随机采样$b$个样本$i_1,i_2,\dots,i_n$来近似损失

    - $$
      \frac{1}{b} \sum_{i\epsilon I_b}l(x_i,y_i,w)
      $$

    - b是批量大小，另一个很重要的==超参数==

    - 选择批量大小比较关键

      - 如果太小，计算量太太小，不适合并行来最大化利用计算资源
      - 如果太大，内存消耗增加，浪费计算

- 总结

  - 梯度下降就是不断沿着反梯度方向更新参数求解
  - 小批量随机梯度下降是深度学习默认的求解算法
  - 两个重要的超参数——批量大小和学习率
